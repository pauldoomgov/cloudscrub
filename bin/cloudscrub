#!/usr/bin/env ruby

require 'bundler/setup'
require 'filesize'
require 'logger'
require 'optionparser'
require 'cloudscrub'

def main
  options = {}

  # Defaults
  options[:local_dir] = '.'
  options[:log_level] = Logger::INFO
  options[:dry_run] = false

  myname = File.basename($0)
  optparse = OptionParser.new do |opts|
    opts.banner = <<-EOM
usage: #{myname} [OPTIONS] LOG_GROUP_NAME

Download all CloudWatch Logs for the specified time window from the specified log group,
and upload them to LOG_BUCKET_NAME, then delete the original log stream, The current 
directory will be used for temporary storage of the log files, which may be very large!

Filtering is applied per-log line.  Only one regex filter may be specified and should
be in the form of a search/replace.  Multiple JSON Path filters may be specified.  If a
JSON Path filter is set the log line will be parsed as JSON and all elements matching a
JSON Path filter will be removed from the output.
See https://goessner.net/articles/JsonPath/ for more on JSON Path.  Experiment with
it here: http://jsonpath.com/

Examples:

# List streams in the /var/log/messages group with events between 2020-01-01 and
# 2020-01-14 23:59 and save in ./slist
#{myname} -s 2020-01-01 -e 2020-01-14T2359 -l -f ./slist /var/log/messages

# Download streams from /var/log/messages listed in streamset and save to the
# directory./cwdump
#{myname} -f ./slist -d ./cwdump -D /var/log/messages

# + Save to a S3 bucket named mylogs under the path arch/:
#{myname} -f ./slist -d ./cwdump -D -b mylogs/arch /var/log/messages

# + Scrub the words "socks" and "hats" as well as the keys ["clothes"]["shirt"] and 
#   ["clothes"]["vest"] from JSON format logs
#{myname} -f ./slist -d ./cwdump -D -b mylogs/arch -g '/socks|hats/' -j '$..clothes.["shirt","vest"]' /var/log/messages

# Same, but delete the source streams where one or more lines match the filters
#{myname} -f ./slist -d ./cwdump -D -b mylogs/arch -g '/socks|hats/' -j '$..clothes.["shirt","vest"]' /var/log/messages

Options:
    EOM

    opts.on('-h', '--help', 'Display this message') do
      STDERR.puts opts
      exit
    end

    opts.on('-s', '--start-time STAMP',
            'Start time in seconds since the epoch or ISO8601 time') do |timestring|
      options[:start_timestring] = timestring
    end

    opts.on('-e', '--end-time STAMP',
            'End time as in seconds since the epoch or ISO8601 time') do |timestring|
      options[:end_timestring] = timestring
    end

    opts.on('-l', '--list-only', 'Write list of streams to STDOUT or stream file and exit') do
      options[:list_only] = true
    end

    opts.on('-d', '--local-dir DIRECTORY', 'Local directory to save logs under') do |local_dir|
      options[:local_dir] = local_dir
    end

    opts.on('-D', '--date-subfolders',
      'Place downloaded and S3 streams under YYYY-MM-DD subfolders') do
      options[:date_subfolders] = true
    end

    opts.on('-f', '--stream-file FILE',
            'Stream list file to write to (-l) or read from') do |stream_file|
      options[:stream_file] = stream_file
    end

    opts.on('-c', '--checkpoint-file FILE',
            'File to track list of completed streams') do |checkpoint_file|
      options[:checkpoint_file] = checkpoint_file
    end

    opts.on('-S', '--skip STREAM',
            'List of log streams (space separated) to ignore') do |streams|
      options[:skip_streams] ||= Set.new
      options[:skip_streams] += streams.split(' ')
    end

    opts.on('-Z', '--split-bytes SIZE',
            'Split output files into chunks <= SIZE - Accepts units like MB or GB') do |val|
      options[:split_bytes] = Filesize.from(val).to_i
      STDERR.puts("\nWill split files at #{options[:split_bytes]} bytes")
    end

    opts.on('-j', '--scrub-jsonpath JSONPATH',
            'One or more JSON Paths (space separated) to redact from logs') do |jsonpaths|
      options[:scrub_jsonpaths] = jsonpaths.split(' ')
      options[:force_json] = true
    end

    opts.on('-J', '--force-json', 'Treat lines as JSON (Automaticly enabled if -j JSONPATH is set)') do
      options[:force_json] = true
    end

    opts.on('-g', '--scrub-gsub PATTERN', 'Ruby gsub (plain or regex) to redact from logs') do |pattern|
      if pattern.start_with?('/') && pattern.end_with?('/')
        options[:scrub_gsub] = pattern.to_regexp
      else
        options[:scrub_gsub] = pattern
      end
    end

    opts.on('-b', '--s3-url URL',
            'S3 URL to store copies of scrubbed logs in') do |s3_url|
      options[:s3_url] = s3_url
    end

    opts.on('--delete', 'Delete stream from CloudWatch once scrubbed version is uploaded to S3') do
      options[:delete] = true
    end

    opts.on('--delete-matching',
            'Delete stream from CW once scrubbed is uploaded to S3 IF the stream matched any patterns') do
      options[:delete_matching] = true
    end

    opts.on('-k', '--keep-local', 'Keep local copies of logs') do
      options[:keep] = true
    end

    opts.on('-n', '--dry-run', "Download and filter but DO NOT upload logs or replace streams") do
      options[:dry_run] = true
    end

    opts.on('--debug', "Enable debug logging") do
      options[:log_level] = Logger::DEBUG
    end

    opts.on('--aws-log FILE', 'Write AWS logs to a file - Useful for measuring AWS API calls') do |aws_log_file|
      options[:aws_log_file] = aws_log_file
    end
  end

  args = optparse.parse(ARGV)

  if args.length != 1
    STDERR.puts(optparse)
    STDERR.puts("\nMust pass a CloudWatch log group name")
    exit 1
  end

  group_name = args[0]

  cs = CloudScrub.new(
              cli_mode: true,
              log_level: options[:log_level],
              aws_log_file: options[:aws_log_file],
              dry_run: options[:dry_run])

  # Coerce time string into milliseconds
  if options[:start_timestring]
    options[:start_time] = cs.timestring_to_stamp_ms(options[:start_timestring])
    cs.log.info("Start time: #{cs.stamp_ms_to_iso8601(options[:start_time])} #{options[:start_time]}ms")
  end

  if options[:end_timestring]
    options[:end_time] = cs.timestring_to_stamp_ms(options[:end_timestring])
    cs.log.info("End time: #{cs.stamp_ms_to_iso8601(options[:end_time])} #{options[:end_time]}ms")
  end

  # Get list of streams
  stream_names = []
  if options[:list_only].nil? && options[:stream_file]
    File.open(options[:stream_file], 'r').each do |l|
      stream_names << l.chomp
    end
    cs.log.info("Loaded #{stream_names.length} streams from #{options[:stream_file]}")
  else
    raw_streams = cs.list_streams(group_name, start_time: options[:start_time], end_time: options[:end_time])

    # We only care about the stream name
    raw_streams.each do |stream|
      stream_names << stream[:log_stream_name]
    end
    cs.log.info("Found #{stream_names.length} streams in group #{group_name.inspect}")
  end

  if !options[:checkpoint_file].nil?
    if File.exist?(options[:checkpoint_file])
      completed = File.read(options[:checkpoint_file]).split.to_set
      if completed.length
        cs.log.info("Adding #{completed.length} streams from #{options[:checkpoint_file].inspect} to skip list")
        options[:skip_streams] ||= Set.new
        options[:skip_streams] += completed
      end
    else
      # Ensure checkpoint file exists without truncating
      File.open(options[:checkpoint_file], 'a') {}
      cs.log.info("Creating checkpoint file #{options[:checkpoint_file].inspect}")
    end
  end

  # Filter streams by name
  if !options[:skip_streams].nil? && options[:skip_streams].length
    slen = stream_names.length
    # Drop streams included in our skip_streams set
    stream_names = stream_names.reject { |s| options[:skip_streams].include?(s) }
    cs.log.info("Will process #{stream_names.length} out of #{slen} detected streams")
  end

  # Dump out stream names
  if options[:list_only]
    if options[:stream_file]
      sfile = File.open(options[:stream_file], 'w')
    else
      sfile = $stdout.dup
    end

    stream_names.each do |stream_name|
      sfile.puts stream_name
    end

    sfile.close

    options[:stream_file].nil? || cs.log.info("Wrote stream list to #{options[:stream_file].inspect}")
    exit 0
  end

  # Directory structure: <local_dir>/<log_group_name>
  if !Dir.exist?(options[:local_dir])
    cs.log.info("Creating local log directory #{options[:local_dir].inspect}")
    Dir.mkdir(options[:local_dir])
  end

  # Process streams
  results = {}

  stream_names.each do |stream_name|
    begin
      # Process stream to filtered file(s) and retain stream to file mapping
      results[stream_name] = cs.download_and_filter_stream(group_name,
                                                           stream_name,
                                                           local_dir: options[:local_dir],
                                                           split_bytes: options[:split_bytes],
                                                           date_subfolders: options[:date_subfolders],
                                                           scrub_gsub: options[:scrub_gsub],
                                                           scrub_jsonpaths: options[:scrub_jsonpaths],
                                                           nest_json: options[:force_json],
                                                          )

      # Stash clean logs in S3
      unless options[:s3_url].nil?
        results[stream_name][:filenames].each do |filename|
          cs.upload_file_to_s3(filename, options[:s3_url], keep_subfolder: options[:date_subfolders])
        end
      end

      # Delete source if asked to, or of any events were scrubbed and asked to delete matchin
      if options[:delete] || (options[:delete_matching] && results[stream_name][:scrubbed_count] > 0)
        cs.delete_cloudwatch_stream(group_name, stream_name)
      elsif options[:delete_matching] && results[stream_name][:scrubbed_count] == 0
        cs.log.info("No matching events in \"#{group_name}/#{stream_name}\"")
      else
        
      end

      # Delete local after processing unless we have no destination or are asked to keep
      if options[:keep].nil? && !options[:s3_url].nil?
        results[stream_name][:filenames].each do |filename|
          File.exist?(filename) && cs.delete_local_file(filename)
        end
      end

      # Mark processed
      if options[:checkpoint_file]
        File.open(options[:checkpoint_file], 'a').write("#{stream_name}\n")
      end
    rescue => err
      cs.log.warn("Unable to process #{stream_name.inspect}: #{err.inspect}")
      cs.log.debug(err.backtrace)

      if results.key?(stream_name)
        # Delete local copy UNLESS we are set to keep local OR or delete
        # streams.  Better safe than without data.
        if !options[:keep].nil? || options[:delete] || options[:delete_matching]
          cs.log.warn("Keeping local files for failed stream #{stream_name.inspect}")
        elsif results[stream_name].key?(:filenames)
          cs.log.warn("Removing local files for #{stream_name.inspect}")
          results[stream_name][:filenames].each do |filename|
            File.exist?(filename) && cs.delete_local_file(filename)
          end
        end
        results.delete(stream_name)
      end
    end
  end

  # Stats stats stats...
  file_count = results.map { |_, s| s[:filenames].length }.sum
  event_count = results.map { |_, s| s[:event_count] }.sum
  scrubbed_count = results.map { |_, s| s[:scrubbed_count] }.sum
  in_bytes = Filesize.new(results.map { |_, s| s[:in_bytes] }.sum).pretty
  out_bytes = Filesize.new(results.map { |_, s| s[:out_bytes] }.sum).pretty

  cs.log.info("#{stream_names.length} streams processed into #{file_count} files with #{event_count} events " +
              "(#{scrubbed_count} scrubbed) - #{in_bytes} read, #{out_bytes} written")
end

if $0 == __FILE__
  main
end
